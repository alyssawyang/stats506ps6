---
title: "Stats 506 PS6"
author: "Alyssa Yang"
format: html
---

**Github repo link**: https://github.com/alyssawyang/stats506ps6.git

# Stratified Bootstrapping
## a
```{r}
#| code-fold: true
library(DBI)
library(RSQLite)
library(dplyr)
library(parallel)
library(future)
library(furrr)
library(tidyr)
library(gt)
library(knitr)

# Connect to database
lahman <- dbConnect(SQLite(), "lahman_1871-2022.sqlite")

# Shortcut for queries
gg <- function(query) {
  dbGetQuery(lahman, query)
}
```

```{r}
# Extract Fielding table
fielding <- gg("SELECT * FROM Fielding")

# Filter fielding table
fielding <- fielding %>%
  filter(!is.na(PO), !is.na(A), !is.na(InnOuts), InnOuts > 0)
```

```{r}
# Calculate RF
fielding$rf <- 3 * ((fielding$PO + fielding$A) / fielding$InnOuts)

# Calculate average RF per team
avg_rf_team <- fielding %>% 
  group_by(teamID) %>% 
  summarise(avg_rf = mean(rf, na.rm = TRUE))

# Render the tibble as an HTML table
kable(avg_rf_team, format = "html")
```

### 1: Without parallel processing
```{r}
#' Non parallel processing bootstrap
#'
#' @param data: data table to work on
#' @param n_samples: number of bootstrap samples
#'
#' @return list of bootstrapped average rf values
boot <- function(data, n_samples) {
  boot_samples <- vector("list", n_samples)
  
  for(i in seq_len(n_samples)) {
    sampled_data <- data %>%
      group_by_at("teamID") %>%
      sample_frac(replace = TRUE) %>% 
      ungroup()
    
    rf <- sampled_data %>% 
      group_by(teamID) %>% 
      summarise(avg_rf = mean(rf, na.rm = TRUE))
    
    boot_samples[[i]] <- rf
  }
  
  bind_rows(boot_samples)
}

# Find bootstrap results of average rf
start_no_parallel <- Sys.time()
boot_results <- boot(fielding, 1000)
end_no_parallel <- Sys.time()

# Find SDs for the results
no_parallel_sd <- boot_results %>% 
  group_by(teamID) %>% 
  summarise(sd_rf = sd(avg_rf, na.rm = TRUE))

# Combine average and SD into a table
table <- avg_rf_team %>% 
  left_join(no_parallel_sd, by = "teamID")

# Render the tibble as an HTML table
kable(table, format = "html")
```

### 2: Parallel processing
```{r}
#' One bootstrap iteration
#'
#' @param data: data table to work on 
#'
#' @return one bootstrap sample of average rf
boot_iter <- function(data) {
  sampled_data <- data %>% 
    group_by_at("teamID") %>%
      sample_frac(replace = TRUE) %>% 
      summarise(avg_rf = mean(rf, na.rm = TRUE))
}
```

```{r}
#' Bootstrap using parallel processing
#'
#' @param data: data table to work on
#' @param n_samples: number of bootstrap samples 
#'
#' @return list of bootstrapped average rf values

parallel_boot <- function(data, n_samples) {
  # Use all cores but one
  n_cores <- detectCores() - 1
  cl <- makeCluster(n_cores)
  
  clusterExport(cl, c("data", "boot_iter"), envir = environment())
  clusterEvalQ(cl, library(dplyr))
  
  boot_samples <- parLapply(cl, 1:n_samples, function(i) boot_iter(data))
  
  stopCluster(cl)
  
  bind_rows(boot_samples)
}

# Find bootstrap results of average rf
start_parallel <- Sys.time()
boot_results <- parallel_boot(fielding, 1000)
end_parallel <- Sys.time()

# Find SDs for the results
parallel_sd <- boot_results %>% 
  group_by(teamID) %>% 
  summarise(sd_rf = sd(avg_rf, na.rm = TRUE))

# Combine average and SD into a table
table_parallel <- avg_rf_team %>% 
  left_join(parallel_sd, by = "teamID")

# Render the tibble as an HTML table
kable(table_parallel, format = "html")
```

### 3: Futures
```{r}
plan(multisession)

start_future <- Sys.time()
boot_results <- future_map_dfr(1:1000, ~ boot_iter(fielding), .options = furrr_options(seed = TRUE))
end_future <- Sys.time()

future_sd <- boot_results %>%
  group_by(teamID) %>%
  summarise(sd_rf = sd(avg_rf, na.rm = TRUE))

table_future <- avg_rf_team %>%
  left_join(future_sd, by = "teamID")

# Render the tibble as an HTML table
kable(table_future, format = "html")
```


## b
```{r}
# Add method columns to each table
table_no_parallel <- table %>%
  mutate(method = "No Parallel")

table_parallel <- table_parallel %>%
  mutate(method = "Parallel")

table_future <- table_future %>%
  mutate(method = "Future")

# Combine all tables
combined_table <- bind_rows(table_no_parallel, table_parallel, table_future)

# Select the top 10 teams by avg_rf across all sources
top_teams <- combined_table %>%
  arrange(desc(avg_rf)) %>%
  group_by(method) %>%
  slice_head(n = 10) %>%
  ungroup()

# Reshape data to wide format
wide_table <- top_teams %>%
  pivot_wider(
    id_cols = teamID,
    names_from = method,
    values_from = c(avg_rf, sd_rf),
    names_sep = " "
  )

# Render the tibble as an HTML table
kable(wide_table, format = "html", table.attr = "style='width:100%;'")
```


## c
```{r}
# Find timings for each method
duration_no_parallel <- end_no_parallel - start_no_parallel
duration_parallel <- end_parallel - start_parallel
duration_futures <- end_future - start_future

# Put all times into a table
timings <- tibble(
  Method = c("No Parallel", "Parallel", "Futures"),
  Duration = c(duration_no_parallel, duration_parallel, duration_futures)
)
# Render the tibble as an HTML table
kable(timings, format = "html")
```

Based on the timings for each method, we can see that stratified bootstrapping with no parallel processing was the slowest method by far, and parallel processing using the parallel and future packages were much faster by more than three times, with both times being pretty similar. All three methods produced very similar standard errors for all teams so there is little performance difference between them - they produce the same results but using parallelization in the two methods significantly reduces evaluation time.
















